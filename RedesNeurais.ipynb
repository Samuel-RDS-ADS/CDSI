{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPN8xpMAXhQ41DA5ITrLbRw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Bibliotecas"
      ],
      "metadata": {
        "id": "KtMyeLkHIQjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "E_t4yNqRHzRn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ligando o Google Drive"
      ],
      "metadata": {
        "id": "_01kIQIvIV2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "caminho = \"/content/drive/MyDrive/CDSI/MaterialApoio/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-aYVB06IPjQ",
        "outputId": "620e679f-81c0-4d54-c00c-b45183b5d8f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base - Credit Data"
      ],
      "metadata": {
        "id": "Fa-qY5ZMYfZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(caminho + 'credit.pkl', 'rb') as f:\n",
        "  X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
      ],
      "metadata": {
        "id": "gajyePm8Yhk_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_treinamento.shape, y_credit_treinamento.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sj6zrIb4YjGX",
        "outputId": "9aff889b-7741-4d46-c18b-ca0fdfe14df2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1500, 3), (1500,))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_teste.shape, y_credit_teste.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzmXe2bpYlC-",
        "outputId": "b788b4f7-4ba1-4c9f-9b25-79d02217ad7e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 3), (500,))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,  solver='adam', activation='relu', hidden_layer_sizes=(10,10))\n",
        "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ADG3yyAOYn4W",
        "outputId": "1c5fc054-c411-4f50-da77-3ac89aa32fc2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.60543267\n",
            "Iteration 2, loss = 0.57429049\n",
            "Iteration 3, loss = 0.54356712\n",
            "Iteration 4, loss = 0.51310474\n",
            "Iteration 5, loss = 0.48353214\n",
            "Iteration 6, loss = 0.45441047\n",
            "Iteration 7, loss = 0.42705607\n",
            "Iteration 8, loss = 0.40057777\n",
            "Iteration 9, loss = 0.37627852\n",
            "Iteration 10, loss = 0.35289615\n",
            "Iteration 11, loss = 0.33124987\n",
            "Iteration 12, loss = 0.31130594\n",
            "Iteration 13, loss = 0.29291973\n",
            "Iteration 14, loss = 0.27637071\n",
            "Iteration 15, loss = 0.26145987\n",
            "Iteration 16, loss = 0.24838016\n",
            "Iteration 17, loss = 0.23639945\n",
            "Iteration 18, loss = 0.22558595\n",
            "Iteration 19, loss = 0.21589089\n",
            "Iteration 20, loss = 0.20700713\n",
            "Iteration 21, loss = 0.19887138\n",
            "Iteration 22, loss = 0.19148656\n",
            "Iteration 23, loss = 0.18477015\n",
            "Iteration 24, loss = 0.17872356\n",
            "Iteration 25, loss = 0.17318478\n",
            "Iteration 26, loss = 0.16812613\n",
            "Iteration 27, loss = 0.16336297\n",
            "Iteration 28, loss = 0.15903522\n",
            "Iteration 29, loss = 0.15491937\n",
            "Iteration 30, loss = 0.15095975\n",
            "Iteration 31, loss = 0.14738578\n",
            "Iteration 32, loss = 0.14391962\n",
            "Iteration 33, loss = 0.14076217\n",
            "Iteration 34, loss = 0.13759868\n",
            "Iteration 35, loss = 0.13455518\n",
            "Iteration 36, loss = 0.13164239\n",
            "Iteration 37, loss = 0.12881064\n",
            "Iteration 38, loss = 0.12612330\n",
            "Iteration 39, loss = 0.12362188\n",
            "Iteration 40, loss = 0.12122840\n",
            "Iteration 41, loss = 0.11905099\n",
            "Iteration 42, loss = 0.11676156\n",
            "Iteration 43, loss = 0.11466744\n",
            "Iteration 44, loss = 0.11276475\n",
            "Iteration 45, loss = 0.11082566\n",
            "Iteration 46, loss = 0.10892162\n",
            "Iteration 47, loss = 0.10704724\n",
            "Iteration 48, loss = 0.10522320\n",
            "Iteration 49, loss = 0.10344956\n",
            "Iteration 50, loss = 0.10170833\n",
            "Iteration 51, loss = 0.09987839\n",
            "Iteration 52, loss = 0.09824818\n",
            "Iteration 53, loss = 0.09646902\n",
            "Iteration 54, loss = 0.09489002\n",
            "Iteration 55, loss = 0.09326357\n",
            "Iteration 56, loss = 0.09178996\n",
            "Iteration 57, loss = 0.09019820\n",
            "Iteration 58, loss = 0.08874643\n",
            "Iteration 59, loss = 0.08744156\n",
            "Iteration 60, loss = 0.08615491\n",
            "Iteration 61, loss = 0.08469004\n",
            "Iteration 62, loss = 0.08334253\n",
            "Iteration 63, loss = 0.08217229\n",
            "Iteration 64, loss = 0.08088776\n",
            "Iteration 65, loss = 0.07969387\n",
            "Iteration 66, loss = 0.07873249\n",
            "Iteration 67, loss = 0.07740733\n",
            "Iteration 68, loss = 0.07627145\n",
            "Iteration 69, loss = 0.07519279\n",
            "Iteration 70, loss = 0.07424909\n",
            "Iteration 71, loss = 0.07319908\n",
            "Iteration 72, loss = 0.07207394\n",
            "Iteration 73, loss = 0.07106174\n",
            "Iteration 74, loss = 0.07001238\n",
            "Iteration 75, loss = 0.06909329\n",
            "Iteration 76, loss = 0.06820513\n",
            "Iteration 77, loss = 0.06723373\n",
            "Iteration 78, loss = 0.06637010\n",
            "Iteration 79, loss = 0.06550188\n",
            "Iteration 80, loss = 0.06455538\n",
            "Iteration 81, loss = 0.06365287\n",
            "Iteration 82, loss = 0.06278359\n",
            "Iteration 83, loss = 0.06200002\n",
            "Iteration 84, loss = 0.06115105\n",
            "Iteration 85, loss = 0.06036756\n",
            "Iteration 86, loss = 0.05957673\n",
            "Iteration 87, loss = 0.05874286\n",
            "Iteration 88, loss = 0.05793395\n",
            "Iteration 89, loss = 0.05742677\n",
            "Iteration 90, loss = 0.05645217\n",
            "Iteration 91, loss = 0.05572935\n",
            "Iteration 92, loss = 0.05511200\n",
            "Iteration 93, loss = 0.05432687\n",
            "Iteration 94, loss = 0.05363218\n",
            "Iteration 95, loss = 0.05289269\n",
            "Iteration 96, loss = 0.05228913\n",
            "Iteration 97, loss = 0.05169976\n",
            "Iteration 98, loss = 0.05099793\n",
            "Iteration 99, loss = 0.05029405\n",
            "Iteration 100, loss = 0.04987842\n",
            "Iteration 101, loss = 0.04915958\n",
            "Iteration 102, loss = 0.04851714\n",
            "Iteration 103, loss = 0.04798188\n",
            "Iteration 104, loss = 0.04740814\n",
            "Iteration 105, loss = 0.04683279\n",
            "Iteration 106, loss = 0.04630633\n",
            "Iteration 107, loss = 0.04578175\n",
            "Iteration 108, loss = 0.04520503\n",
            "Iteration 109, loss = 0.04474028\n",
            "Iteration 110, loss = 0.04429562\n",
            "Iteration 111, loss = 0.04372408\n",
            "Iteration 112, loss = 0.04322137\n",
            "Iteration 113, loss = 0.04266137\n",
            "Iteration 114, loss = 0.04219892\n",
            "Iteration 115, loss = 0.04178246\n",
            "Iteration 116, loss = 0.04126023\n",
            "Iteration 117, loss = 0.04079201\n",
            "Iteration 118, loss = 0.04045383\n",
            "Iteration 119, loss = 0.03992472\n",
            "Iteration 120, loss = 0.03953021\n",
            "Iteration 121, loss = 0.03905199\n",
            "Iteration 122, loss = 0.03864631\n",
            "Iteration 123, loss = 0.03823667\n",
            "Iteration 124, loss = 0.03786848\n",
            "Iteration 125, loss = 0.03744804\n",
            "Iteration 126, loss = 0.03712658\n",
            "Iteration 127, loss = 0.03667887\n",
            "Iteration 128, loss = 0.03644949\n",
            "Iteration 129, loss = 0.03594076\n",
            "Iteration 130, loss = 0.03559463\n",
            "Iteration 131, loss = 0.03525208\n",
            "Iteration 132, loss = 0.03492535\n",
            "Iteration 133, loss = 0.03451584\n",
            "Iteration 134, loss = 0.03416705\n",
            "Iteration 135, loss = 0.03388904\n",
            "Iteration 136, loss = 0.03354183\n",
            "Iteration 137, loss = 0.03324989\n",
            "Iteration 138, loss = 0.03291751\n",
            "Iteration 139, loss = 0.03258057\n",
            "Iteration 140, loss = 0.03230833\n",
            "Iteration 141, loss = 0.03194450\n",
            "Iteration 142, loss = 0.03187433\n",
            "Iteration 143, loss = 0.03146978\n",
            "Iteration 144, loss = 0.03105188\n",
            "Iteration 145, loss = 0.03080024\n",
            "Iteration 146, loss = 0.03047944\n",
            "Iteration 147, loss = 0.03024326\n",
            "Iteration 148, loss = 0.02997124\n",
            "Iteration 149, loss = 0.02970031\n",
            "Iteration 150, loss = 0.02944205\n",
            "Iteration 151, loss = 0.02913514\n",
            "Iteration 152, loss = 0.02894454\n",
            "Iteration 153, loss = 0.02873728\n",
            "Iteration 154, loss = 0.02839354\n",
            "Iteration 155, loss = 0.02827305\n",
            "Iteration 156, loss = 0.02791887\n",
            "Iteration 157, loss = 0.02773207\n",
            "Iteration 158, loss = 0.02747575\n",
            "Iteration 159, loss = 0.02731334\n",
            "Iteration 160, loss = 0.02704972\n",
            "Iteration 161, loss = 0.02684952\n",
            "Iteration 162, loss = 0.02659719\n",
            "Iteration 163, loss = 0.02641791\n",
            "Iteration 164, loss = 0.02621725\n",
            "Iteration 165, loss = 0.02602499\n",
            "Iteration 166, loss = 0.02582464\n",
            "Iteration 167, loss = 0.02562170\n",
            "Iteration 168, loss = 0.02540245\n",
            "Iteration 169, loss = 0.02525657\n",
            "Iteration 170, loss = 0.02504967\n",
            "Iteration 171, loss = 0.02484199\n",
            "Iteration 172, loss = 0.02467786\n",
            "Iteration 173, loss = 0.02461986\n",
            "Iteration 174, loss = 0.02430292\n",
            "Iteration 175, loss = 0.02411564\n",
            "Iteration 176, loss = 0.02397565\n",
            "Iteration 177, loss = 0.02374816\n",
            "Iteration 178, loss = 0.02354542\n",
            "Iteration 179, loss = 0.02344247\n",
            "Iteration 180, loss = 0.02332017\n",
            "Iteration 181, loss = 0.02301226\n",
            "Iteration 182, loss = 0.02293025\n",
            "Iteration 183, loss = 0.02271234\n",
            "Iteration 184, loss = 0.02252147\n",
            "Iteration 185, loss = 0.02239784\n",
            "Iteration 186, loss = 0.02228257\n",
            "Iteration 187, loss = 0.02210807\n",
            "Iteration 188, loss = 0.02194627\n",
            "Iteration 189, loss = 0.02178003\n",
            "Iteration 190, loss = 0.02180397\n",
            "Iteration 191, loss = 0.02149521\n",
            "Iteration 192, loss = 0.02136188\n",
            "Iteration 193, loss = 0.02128366\n",
            "Iteration 194, loss = 0.02113444\n",
            "Iteration 195, loss = 0.02095608\n",
            "Iteration 196, loss = 0.02081085\n",
            "Iteration 197, loss = 0.02068172\n",
            "Iteration 198, loss = 0.02055492\n",
            "Iteration 199, loss = 0.02049927\n",
            "Iteration 200, loss = 0.02028844\n",
            "Iteration 201, loss = 0.02021599\n",
            "Iteration 202, loss = 0.02006487\n",
            "Iteration 203, loss = 0.01995888\n",
            "Iteration 204, loss = 0.01986273\n",
            "Iteration 205, loss = 0.01975574\n",
            "Iteration 206, loss = 0.01961220\n",
            "Iteration 207, loss = 0.01949810\n",
            "Iteration 208, loss = 0.01943632\n",
            "Iteration 209, loss = 0.01930690\n",
            "Iteration 210, loss = 0.01914557\n",
            "Iteration 211, loss = 0.01912479\n",
            "Iteration 212, loss = 0.01893996\n",
            "Iteration 213, loss = 0.01893606\n",
            "Iteration 214, loss = 0.01873342\n",
            "Iteration 215, loss = 0.01859577\n",
            "Iteration 216, loss = 0.01856065\n",
            "Iteration 217, loss = 0.01854809\n",
            "Iteration 218, loss = 0.01834686\n",
            "Iteration 219, loss = 0.01824510\n",
            "Iteration 220, loss = 0.01816639\n",
            "Iteration 221, loss = 0.01804220\n",
            "Iteration 222, loss = 0.01792802\n",
            "Iteration 223, loss = 0.01784455\n",
            "Iteration 224, loss = 0.01774841\n",
            "Iteration 225, loss = 0.01765294\n",
            "Iteration 226, loss = 0.01756944\n",
            "Iteration 227, loss = 0.01752223\n",
            "Iteration 228, loss = 0.01741925\n",
            "Iteration 229, loss = 0.01739877\n",
            "Iteration 230, loss = 0.01721468\n",
            "Iteration 231, loss = 0.01712453\n",
            "Iteration 232, loss = 0.01709131\n",
            "Iteration 233, loss = 0.01692893\n",
            "Iteration 234, loss = 0.01685442\n",
            "Iteration 235, loss = 0.01687722\n",
            "Iteration 236, loss = 0.01666576\n",
            "Iteration 237, loss = 0.01663040\n",
            "Iteration 238, loss = 0.01658926\n",
            "Iteration 239, loss = 0.01650363\n",
            "Iteration 240, loss = 0.01638455\n",
            "Iteration 241, loss = 0.01633194\n",
            "Iteration 242, loss = 0.01620505\n",
            "Iteration 243, loss = 0.01623062\n",
            "Iteration 244, loss = 0.01616192\n",
            "Iteration 245, loss = 0.01603455\n",
            "Iteration 246, loss = 0.01593981\n",
            "Iteration 247, loss = 0.01579361\n",
            "Iteration 248, loss = 0.01576808\n",
            "Iteration 249, loss = 0.01572321\n",
            "Iteration 250, loss = 0.01565505\n",
            "Iteration 251, loss = 0.01564059\n",
            "Iteration 252, loss = 0.01545389\n",
            "Iteration 253, loss = 0.01551482\n",
            "Iteration 254, loss = 0.01553201\n",
            "Iteration 255, loss = 0.01527444\n",
            "Iteration 256, loss = 0.01521014\n",
            "Iteration 257, loss = 0.01517706\n",
            "Iteration 258, loss = 0.01500548\n",
            "Iteration 259, loss = 0.01498803\n",
            "Iteration 260, loss = 0.01495265\n",
            "Iteration 261, loss = 0.01488046\n",
            "Iteration 262, loss = 0.01480541\n",
            "Iteration 263, loss = 0.01479382\n",
            "Iteration 264, loss = 0.01465932\n",
            "Iteration 265, loss = 0.01457303\n",
            "Iteration 266, loss = 0.01455470\n",
            "Iteration 267, loss = 0.01442777\n",
            "Iteration 268, loss = 0.01453060\n",
            "Iteration 269, loss = 0.01438660\n",
            "Iteration 270, loss = 0.01425409\n",
            "Iteration 271, loss = 0.01426718\n",
            "Iteration 272, loss = 0.01414365\n",
            "Iteration 273, loss = 0.01406964\n",
            "Iteration 274, loss = 0.01399317\n",
            "Iteration 275, loss = 0.01392854\n",
            "Iteration 276, loss = 0.01388455\n",
            "Iteration 277, loss = 0.01386189\n",
            "Iteration 278, loss = 0.01378780\n",
            "Iteration 279, loss = 0.01377527\n",
            "Iteration 280, loss = 0.01366864\n",
            "Iteration 281, loss = 0.01359845\n",
            "Iteration 282, loss = 0.01351576\n",
            "Iteration 283, loss = 0.01343305\n",
            "Iteration 284, loss = 0.01350383\n",
            "Iteration 285, loss = 0.01331935\n",
            "Iteration 286, loss = 0.01333074\n",
            "Iteration 287, loss = 0.01323157\n",
            "Iteration 288, loss = 0.01314826\n",
            "Iteration 289, loss = 0.01313829\n",
            "Iteration 290, loss = 0.01306144\n",
            "Iteration 291, loss = 0.01296969\n",
            "Iteration 292, loss = 0.01294570\n",
            "Iteration 293, loss = 0.01293379\n",
            "Iteration 294, loss = 0.01287689\n",
            "Iteration 295, loss = 0.01278502\n",
            "Iteration 296, loss = 0.01285750\n",
            "Iteration 297, loss = 0.01265015\n",
            "Iteration 298, loss = 0.01266248\n",
            "Iteration 299, loss = 0.01255767\n",
            "Iteration 300, loss = 0.01248134\n",
            "Iteration 301, loss = 0.01242199\n",
            "Iteration 302, loss = 0.01236059\n",
            "Iteration 303, loss = 0.01231233\n",
            "Iteration 304, loss = 0.01227940\n",
            "Iteration 305, loss = 0.01218984\n",
            "Iteration 306, loss = 0.01217590\n",
            "Iteration 307, loss = 0.01208721\n",
            "Iteration 308, loss = 0.01205088\n",
            "Iteration 309, loss = 0.01208851\n",
            "Iteration 310, loss = 0.01195081\n",
            "Iteration 311, loss = 0.01190558\n",
            "Iteration 312, loss = 0.01208206\n",
            "Iteration 313, loss = 0.01183364\n",
            "Iteration 314, loss = 0.01183249\n",
            "Iteration 315, loss = 0.01166594\n",
            "Iteration 316, loss = 0.01169295\n",
            "Iteration 317, loss = 0.01166301\n",
            "Iteration 318, loss = 0.01153804\n",
            "Iteration 319, loss = 0.01166951\n",
            "Iteration 320, loss = 0.01148193\n",
            "Iteration 321, loss = 0.01141302\n",
            "Iteration 322, loss = 0.01136956\n",
            "Iteration 323, loss = 0.01135503\n",
            "Iteration 324, loss = 0.01127573\n",
            "Iteration 325, loss = 0.01123776\n",
            "Iteration 326, loss = 0.01129534\n",
            "Iteration 327, loss = 0.01130750\n",
            "Iteration 328, loss = 0.01115429\n",
            "Iteration 329, loss = 0.01105658\n",
            "Iteration 330, loss = 0.01102862\n",
            "Iteration 331, loss = 0.01092506\n",
            "Iteration 332, loss = 0.01103030\n",
            "Iteration 333, loss = 0.01094907\n",
            "Iteration 334, loss = 0.01082778\n",
            "Iteration 335, loss = 0.01080501\n",
            "Iteration 336, loss = 0.01076437\n",
            "Iteration 337, loss = 0.01077732\n",
            "Iteration 338, loss = 0.01064542\n",
            "Iteration 339, loss = 0.01063239\n",
            "Iteration 340, loss = 0.01062559\n",
            "Iteration 341, loss = 0.01059756\n",
            "Iteration 342, loss = 0.01052116\n",
            "Iteration 343, loss = 0.01051370\n",
            "Iteration 344, loss = 0.01054068\n",
            "Iteration 345, loss = 0.01045552\n",
            "Iteration 346, loss = 0.01036970\n",
            "Iteration 347, loss = 0.01032080\n",
            "Iteration 348, loss = 0.01028894\n",
            "Iteration 349, loss = 0.01020772\n",
            "Iteration 350, loss = 0.01016516\n",
            "Iteration 351, loss = 0.01014420\n",
            "Iteration 352, loss = 0.01017907\n",
            "Iteration 353, loss = 0.01022394\n",
            "Iteration 354, loss = 0.01003805\n",
            "Iteration 355, loss = 0.01003973\n",
            "Iteration 356, loss = 0.00994371\n",
            "Iteration 357, loss = 0.00994741\n",
            "Iteration 358, loss = 0.00987612\n",
            "Iteration 359, loss = 0.00984299\n",
            "Iteration 360, loss = 0.00984190\n",
            "Iteration 361, loss = 0.00975654\n",
            "Iteration 362, loss = 0.00974518\n",
            "Iteration 363, loss = 0.00971608\n",
            "Iteration 364, loss = 0.00970350\n",
            "Iteration 365, loss = 0.00961979\n",
            "Iteration 366, loss = 0.00956695\n",
            "Iteration 367, loss = 0.00957174\n",
            "Iteration 368, loss = 0.00951513\n",
            "Iteration 369, loss = 0.00953667\n",
            "Iteration 370, loss = 0.00945792\n",
            "Iteration 371, loss = 0.00943022\n",
            "Iteration 372, loss = 0.00944650\n",
            "Iteration 373, loss = 0.00935953\n",
            "Iteration 374, loss = 0.00928892\n",
            "Iteration 375, loss = 0.00932030\n",
            "Iteration 376, loss = 0.00926143\n",
            "Iteration 377, loss = 0.00919402\n",
            "Iteration 378, loss = 0.00915875\n",
            "Iteration 379, loss = 0.00912225\n",
            "Iteration 380, loss = 0.00913482\n",
            "Iteration 381, loss = 0.00908633\n",
            "Iteration 382, loss = 0.00911494\n",
            "Iteration 383, loss = 0.00904506\n",
            "Iteration 384, loss = 0.00897356\n",
            "Iteration 385, loss = 0.00899035\n",
            "Iteration 386, loss = 0.00900619\n",
            "Iteration 387, loss = 0.00903365\n",
            "Iteration 388, loss = 0.00884925\n",
            "Iteration 389, loss = 0.00883268\n",
            "Iteration 390, loss = 0.00893887\n",
            "Iteration 391, loss = 0.00901634\n",
            "Iteration 392, loss = 0.00881336\n",
            "Iteration 393, loss = 0.00868679\n",
            "Iteration 394, loss = 0.00865927\n",
            "Iteration 395, loss = 0.00861630\n",
            "Iteration 396, loss = 0.00860932\n",
            "Iteration 397, loss = 0.00863986\n",
            "Iteration 398, loss = 0.00867270\n",
            "Iteration 399, loss = 0.00853260\n",
            "Iteration 400, loss = 0.00851383\n",
            "Iteration 401, loss = 0.00848350\n",
            "Iteration 402, loss = 0.00842677\n",
            "Iteration 403, loss = 0.00839049\n",
            "Iteration 404, loss = 0.00834289\n",
            "Iteration 405, loss = 0.00831973\n",
            "Iteration 406, loss = 0.00831771\n",
            "Iteration 407, loss = 0.00827684\n",
            "Iteration 408, loss = 0.00823030\n",
            "Iteration 409, loss = 0.00823564\n",
            "Iteration 410, loss = 0.00815727\n",
            "Iteration 411, loss = 0.00828053\n",
            "Iteration 412, loss = 0.00816878\n",
            "Iteration 413, loss = 0.00813068\n",
            "Iteration 414, loss = 0.00808550\n",
            "Iteration 415, loss = 0.00806265\n",
            "Iteration 416, loss = 0.00804616\n",
            "Iteration 417, loss = 0.00798379\n",
            "Iteration 418, loss = 0.00794965\n",
            "Iteration 419, loss = 0.00797274\n",
            "Iteration 420, loss = 0.00788187\n",
            "Iteration 421, loss = 0.00790694\n",
            "Iteration 422, loss = 0.00786851\n",
            "Iteration 423, loss = 0.00782729\n",
            "Iteration 424, loss = 0.00779523\n",
            "Iteration 425, loss = 0.00775044\n",
            "Iteration 426, loss = 0.00773894\n",
            "Iteration 427, loss = 0.00769494\n",
            "Iteration 428, loss = 0.00770211\n",
            "Iteration 429, loss = 0.00773686\n",
            "Iteration 430, loss = 0.00764742\n",
            "Iteration 431, loss = 0.00764367\n",
            "Iteration 432, loss = 0.00757708\n",
            "Iteration 433, loss = 0.00761044\n",
            "Iteration 434, loss = 0.00755368\n",
            "Iteration 435, loss = 0.00752416\n",
            "Iteration 436, loss = 0.00751323\n",
            "Iteration 437, loss = 0.00746611\n",
            "Iteration 438, loss = 0.00752249\n",
            "Iteration 439, loss = 0.00742194\n",
            "Iteration 440, loss = 0.00754703\n",
            "Iteration 441, loss = 0.00749990\n",
            "Iteration 442, loss = 0.00745239\n",
            "Iteration 443, loss = 0.00728369\n",
            "Iteration 444, loss = 0.00727920\n",
            "Iteration 445, loss = 0.00733933\n",
            "Iteration 446, loss = 0.00726802\n",
            "Iteration 447, loss = 0.00724872\n",
            "Iteration 448, loss = 0.00720132\n",
            "Iteration 449, loss = 0.00718423\n",
            "Iteration 450, loss = 0.00716182\n",
            "Iteration 451, loss = 0.00716632\n",
            "Iteration 452, loss = 0.00709645\n",
            "Iteration 453, loss = 0.00707913\n",
            "Iteration 454, loss = 0.00704848\n",
            "Iteration 455, loss = 0.00709338\n",
            "Iteration 456, loss = 0.00709808\n",
            "Iteration 457, loss = 0.00698985\n",
            "Iteration 458, loss = 0.00695317\n",
            "Iteration 459, loss = 0.00692433\n",
            "Iteration 460, loss = 0.00694726\n",
            "Iteration 461, loss = 0.00690691\n",
            "Iteration 462, loss = 0.00683738\n",
            "Iteration 463, loss = 0.00686872\n",
            "Iteration 464, loss = 0.00684622\n",
            "Iteration 465, loss = 0.00681525\n",
            "Iteration 466, loss = 0.00677261\n",
            "Iteration 467, loss = 0.00673227\n",
            "Iteration 468, loss = 0.00675320\n",
            "Iteration 469, loss = 0.00673390\n",
            "Iteration 470, loss = 0.00671669\n",
            "Iteration 471, loss = 0.00669885\n",
            "Iteration 472, loss = 0.00662552\n",
            "Iteration 473, loss = 0.00667796\n",
            "Iteration 474, loss = 0.00659458\n",
            "Iteration 475, loss = 0.00654925\n",
            "Iteration 476, loss = 0.00654891\n",
            "Iteration 477, loss = 0.00653695\n",
            "Iteration 478, loss = 0.00649944\n",
            "Iteration 479, loss = 0.00651240\n",
            "Iteration 480, loss = 0.00648661\n",
            "Iteration 481, loss = 0.00644800\n",
            "Iteration 482, loss = 0.00644170\n",
            "Iteration 483, loss = 0.00648634\n",
            "Iteration 484, loss = 0.00655103\n",
            "Iteration 485, loss = 0.00634914\n",
            "Iteration 486, loss = 0.00639613\n",
            "Iteration 487, loss = 0.00641619\n",
            "Iteration 488, loss = 0.00641436\n",
            "Iteration 489, loss = 0.00637193\n",
            "Iteration 490, loss = 0.00626515\n",
            "Iteration 491, loss = 0.00624336\n",
            "Iteration 492, loss = 0.00622256\n",
            "Iteration 493, loss = 0.00620295\n",
            "Iteration 494, loss = 0.00621635\n",
            "Iteration 495, loss = 0.00615185\n",
            "Iteration 496, loss = 0.00617311\n",
            "Iteration 497, loss = 0.00614239\n",
            "Iteration 498, loss = 0.00614177\n",
            "Iteration 499, loss = 0.00611911\n",
            "Iteration 500, loss = 0.00607539\n",
            "Iteration 501, loss = 0.00603208\n",
            "Iteration 502, loss = 0.00602419\n",
            "Iteration 503, loss = 0.00603746\n",
            "Iteration 504, loss = 0.00600496\n",
            "Iteration 505, loss = 0.00599639\n",
            "Iteration 506, loss = 0.00596950\n",
            "Iteration 507, loss = 0.00599525\n",
            "Iteration 508, loss = 0.00593117\n",
            "Iteration 509, loss = 0.00609132\n",
            "Iteration 510, loss = 0.00588753\n",
            "Iteration 511, loss = 0.00583533\n",
            "Iteration 512, loss = 0.00587224\n",
            "Iteration 513, loss = 0.00588732\n",
            "Iteration 514, loss = 0.00583347\n",
            "Iteration 515, loss = 0.00578280\n",
            "Iteration 516, loss = 0.00579447\n",
            "Iteration 517, loss = 0.00574656\n",
            "Iteration 518, loss = 0.00575139\n",
            "Iteration 519, loss = 0.00576382\n",
            "Iteration 520, loss = 0.00572758\n",
            "Iteration 521, loss = 0.00565585\n",
            "Iteration 522, loss = 0.00569699\n",
            "Iteration 523, loss = 0.00567040\n",
            "Iteration 524, loss = 0.00562634\n",
            "Iteration 525, loss = 0.00560209\n",
            "Iteration 526, loss = 0.00563448\n",
            "Iteration 527, loss = 0.00560653\n",
            "Iteration 528, loss = 0.00561676\n",
            "Iteration 529, loss = 0.00555590\n",
            "Iteration 530, loss = 0.00555807\n",
            "Iteration 531, loss = 0.00555240\n",
            "Iteration 532, loss = 0.00557359\n",
            "Iteration 533, loss = 0.00548480\n",
            "Iteration 534, loss = 0.00547849\n",
            "Iteration 535, loss = 0.00543482\n",
            "Iteration 536, loss = 0.00545917\n",
            "Iteration 537, loss = 0.00539698\n",
            "Iteration 538, loss = 0.00541574\n",
            "Iteration 539, loss = 0.00539609\n",
            "Iteration 540, loss = 0.00538380\n",
            "Iteration 541, loss = 0.00545450\n",
            "Iteration 542, loss = 0.00534373\n",
            "Iteration 543, loss = 0.00534778\n",
            "Iteration 544, loss = 0.00537115\n",
            "Iteration 545, loss = 0.00528246\n",
            "Iteration 546, loss = 0.00528072\n",
            "Iteration 547, loss = 0.00523169\n",
            "Iteration 548, loss = 0.00524946\n",
            "Iteration 549, loss = 0.00521651\n",
            "Iteration 550, loss = 0.00519373\n",
            "Iteration 551, loss = 0.00518844\n",
            "Iteration 552, loss = 0.00513493\n",
            "Iteration 553, loss = 0.00516177\n",
            "Iteration 554, loss = 0.00522281\n",
            "Iteration 555, loss = 0.00513327\n",
            "Iteration 556, loss = 0.00510396\n",
            "Iteration 557, loss = 0.00519562\n",
            "Iteration 558, loss = 0.00503977\n",
            "Iteration 559, loss = 0.00514550\n",
            "Iteration 560, loss = 0.00503023\n",
            "Iteration 561, loss = 0.00503972\n",
            "Iteration 562, loss = 0.00501173\n",
            "Iteration 563, loss = 0.00500133\n",
            "Iteration 564, loss = 0.00503818\n",
            "Iteration 565, loss = 0.00495818\n",
            "Iteration 566, loss = 0.00496589\n",
            "Iteration 567, loss = 0.00501376\n",
            "Iteration 568, loss = 0.00492964\n",
            "Iteration 569, loss = 0.00489423\n",
            "Iteration 570, loss = 0.00491428\n",
            "Iteration 571, loss = 0.00486278\n",
            "Iteration 572, loss = 0.00488573\n",
            "Iteration 573, loss = 0.00485786\n",
            "Iteration 574, loss = 0.00489306\n",
            "Iteration 575, loss = 0.00483483\n",
            "Iteration 576, loss = 0.00482182\n",
            "Iteration 577, loss = 0.00479606\n",
            "Iteration 578, loss = 0.00475358\n",
            "Iteration 579, loss = 0.00479031\n",
            "Iteration 580, loss = 0.00473235\n",
            "Iteration 581, loss = 0.00478934\n",
            "Iteration 582, loss = 0.00473718\n",
            "Iteration 583, loss = 0.00471219\n",
            "Iteration 584, loss = 0.00473309\n",
            "Iteration 585, loss = 0.00472995\n",
            "Iteration 586, loss = 0.00469837\n",
            "Iteration 587, loss = 0.00468222\n",
            "Iteration 588, loss = 0.00464370\n",
            "Iteration 589, loss = 0.00465333\n",
            "Iteration 590, loss = 0.00465186\n",
            "Iteration 591, loss = 0.00463131\n",
            "Iteration 592, loss = 0.00458299\n",
            "Iteration 593, loss = 0.00461740\n",
            "Iteration 594, loss = 0.00456443\n",
            "Iteration 595, loss = 0.00457286\n",
            "Iteration 596, loss = 0.00454163\n",
            "Iteration 597, loss = 0.00452795\n",
            "Iteration 598, loss = 0.00454785\n",
            "Iteration 599, loss = 0.00457607\n",
            "Iteration 600, loss = 0.00450844\n",
            "Iteration 601, loss = 0.00450108\n",
            "Iteration 602, loss = 0.00449293\n",
            "Iteration 603, loss = 0.00447194\n",
            "Iteration 604, loss = 0.00448866\n",
            "Iteration 605, loss = 0.00444802\n",
            "Iteration 606, loss = 0.00446000\n",
            "Iteration 607, loss = 0.00440060\n",
            "Iteration 608, loss = 0.00443112\n",
            "Iteration 609, loss = 0.00441450\n",
            "Iteration 610, loss = 0.00441225\n",
            "Iteration 611, loss = 0.00435172\n",
            "Iteration 612, loss = 0.00434255\n",
            "Iteration 613, loss = 0.00433428\n",
            "Iteration 614, loss = 0.00432888\n",
            "Iteration 615, loss = 0.00433621\n",
            "Iteration 616, loss = 0.00432938\n",
            "Iteration 617, loss = 0.00435418\n",
            "Iteration 618, loss = 0.00427003\n",
            "Iteration 619, loss = 0.00428289\n",
            "Iteration 620, loss = 0.00428491\n",
            "Iteration 621, loss = 0.00429591\n",
            "Iteration 622, loss = 0.00425199\n",
            "Iteration 623, loss = 0.00429119\n",
            "Iteration 624, loss = 0.00422066\n",
            "Iteration 625, loss = 0.00418593\n",
            "Iteration 626, loss = 0.00419625\n",
            "Iteration 627, loss = 0.00421481\n",
            "Iteration 628, loss = 0.00418407\n",
            "Iteration 629, loss = 0.00416459\n",
            "Iteration 630, loss = 0.00415128\n",
            "Iteration 631, loss = 0.00412233\n",
            "Iteration 632, loss = 0.00416194\n",
            "Iteration 633, loss = 0.00411718\n",
            "Iteration 634, loss = 0.00413588\n",
            "Iteration 635, loss = 0.00411613\n",
            "Iteration 636, loss = 0.00407266\n",
            "Iteration 637, loss = 0.00403397\n",
            "Iteration 638, loss = 0.00414389\n",
            "Iteration 639, loss = 0.00402215\n",
            "Iteration 640, loss = 0.00409891\n",
            "Iteration 641, loss = 0.00413292\n",
            "Iteration 642, loss = 0.00405032\n",
            "Iteration 643, loss = 0.00399358\n",
            "Iteration 644, loss = 0.00400336\n",
            "Iteration 645, loss = 0.00404455\n",
            "Iteration 646, loss = 0.00400420\n",
            "Iteration 647, loss = 0.00401695\n",
            "Iteration 648, loss = 0.00389792\n",
            "Iteration 649, loss = 0.00399405\n",
            "Iteration 650, loss = 0.00393146\n",
            "Iteration 651, loss = 0.00390311\n",
            "Iteration 652, loss = 0.00394585\n",
            "Iteration 653, loss = 0.00382480\n",
            "Iteration 654, loss = 0.00388873\n",
            "Iteration 655, loss = 0.00387674\n",
            "Iteration 656, loss = 0.00387754\n",
            "Iteration 657, loss = 0.00384820\n",
            "Iteration 658, loss = 0.00381068\n",
            "Iteration 659, loss = 0.00382282\n",
            "Iteration 660, loss = 0.00383968\n",
            "Iteration 661, loss = 0.00379944\n",
            "Iteration 662, loss = 0.00388502\n",
            "Iteration 663, loss = 0.00375753\n",
            "Iteration 664, loss = 0.00377690\n",
            "Iteration 665, loss = 0.00376137\n",
            "Iteration 666, loss = 0.00375571\n",
            "Iteration 667, loss = 0.00377270\n",
            "Iteration 668, loss = 0.00371003\n",
            "Iteration 669, loss = 0.00373720\n",
            "Iteration 670, loss = 0.00373430\n",
            "Iteration 671, loss = 0.00367836\n",
            "Iteration 672, loss = 0.00368895\n",
            "Iteration 673, loss = 0.00369477\n",
            "Iteration 674, loss = 0.00367670\n",
            "Iteration 675, loss = 0.00366478\n",
            "Iteration 676, loss = 0.00363129\n",
            "Iteration 677, loss = 0.00366301\n",
            "Iteration 678, loss = 0.00363397\n",
            "Iteration 679, loss = 0.00359628\n",
            "Iteration 680, loss = 0.00364834\n",
            "Iteration 681, loss = 0.00360045\n",
            "Iteration 682, loss = 0.00362639\n",
            "Iteration 683, loss = 0.00360270\n",
            "Iteration 684, loss = 0.00357862\n",
            "Iteration 685, loss = 0.00357568\n",
            "Iteration 686, loss = 0.00349153\n",
            "Iteration 687, loss = 0.00358266\n",
            "Iteration 688, loss = 0.00353935\n",
            "Iteration 689, loss = 0.00353256\n",
            "Iteration 690, loss = 0.00352774\n",
            "Iteration 691, loss = 0.00353803\n",
            "Iteration 692, loss = 0.00347679\n",
            "Iteration 693, loss = 0.00350059\n",
            "Iteration 694, loss = 0.00346364\n",
            "Iteration 695, loss = 0.00357199\n",
            "Iteration 696, loss = 0.00348265\n",
            "Iteration 697, loss = 0.00344307\n",
            "Iteration 698, loss = 0.00342016\n",
            "Iteration 699, loss = 0.00341591\n",
            "Iteration 700, loss = 0.00345871\n",
            "Iteration 701, loss = 0.00340126\n",
            "Iteration 702, loss = 0.00342765\n",
            "Iteration 703, loss = 0.00342265\n",
            "Iteration 704, loss = 0.00341567\n",
            "Iteration 705, loss = 0.00348007\n",
            "Iteration 706, loss = 0.00343553\n",
            "Iteration 707, loss = 0.00341750\n",
            "Iteration 708, loss = 0.00339913\n",
            "Iteration 709, loss = 0.00337815\n",
            "Iteration 710, loss = 0.00339490\n",
            "Iteration 711, loss = 0.00331241\n",
            "Iteration 712, loss = 0.00335453\n",
            "Iteration 713, loss = 0.00332073\n",
            "Iteration 714, loss = 0.00332412\n",
            "Iteration 715, loss = 0.00330489\n",
            "Iteration 716, loss = 0.00324922\n",
            "Iteration 717, loss = 0.00327117\n",
            "Iteration 718, loss = 0.00326559\n",
            "Iteration 719, loss = 0.00326482\n",
            "Iteration 720, loss = 0.00322100\n",
            "Iteration 721, loss = 0.00324869\n",
            "Iteration 722, loss = 0.00325481\n",
            "Iteration 723, loss = 0.00326541\n",
            "Iteration 724, loss = 0.00319206\n",
            "Iteration 725, loss = 0.00324214\n",
            "Iteration 726, loss = 0.00321676\n",
            "Iteration 727, loss = 0.00320492\n",
            "Iteration 728, loss = 0.00318123\n",
            "Iteration 729, loss = 0.00325528\n",
            "Iteration 730, loss = 0.00319461\n",
            "Iteration 731, loss = 0.00319253\n",
            "Iteration 732, loss = 0.00322001\n",
            "Iteration 733, loss = 0.00317583\n",
            "Iteration 734, loss = 0.00314120\n",
            "Iteration 735, loss = 0.00312800\n",
            "Iteration 736, loss = 0.00309953\n",
            "Iteration 737, loss = 0.00319730\n",
            "Iteration 738, loss = 0.00324827\n",
            "Iteration 739, loss = 0.00317323\n",
            "Iteration 740, loss = 0.00315746\n",
            "Iteration 741, loss = 0.00309606\n",
            "Iteration 742, loss = 0.00307596\n",
            "Iteration 743, loss = 0.00310297\n",
            "Iteration 744, loss = 0.00305604\n",
            "Iteration 745, loss = 0.00306290\n",
            "Iteration 746, loss = 0.00303672\n",
            "Iteration 747, loss = 0.00305774\n",
            "Iteration 748, loss = 0.00299172\n",
            "Iteration 749, loss = 0.00307375\n",
            "Iteration 750, loss = 0.00301291\n",
            "Iteration 751, loss = 0.00299856\n",
            "Iteration 752, loss = 0.00302417\n",
            "Iteration 753, loss = 0.00295661\n",
            "Iteration 754, loss = 0.00299328\n",
            "Iteration 755, loss = 0.00297794\n",
            "Iteration 756, loss = 0.00293982\n",
            "Iteration 757, loss = 0.00294846\n",
            "Iteration 758, loss = 0.00305416\n",
            "Iteration 759, loss = 0.00305390\n",
            "Iteration 760, loss = 0.00293988\n",
            "Iteration 761, loss = 0.00304096\n",
            "Iteration 762, loss = 0.00292095\n",
            "Iteration 763, loss = 0.00291612\n",
            "Iteration 764, loss = 0.00289646\n",
            "Iteration 765, loss = 0.00291521\n",
            "Iteration 766, loss = 0.00288128\n",
            "Iteration 767, loss = 0.00287869\n",
            "Iteration 768, loss = 0.00290462\n",
            "Iteration 769, loss = 0.00286029\n",
            "Iteration 770, loss = 0.00287081\n",
            "Iteration 771, loss = 0.00282836\n",
            "Iteration 772, loss = 0.00283402\n",
            "Iteration 773, loss = 0.00282538\n",
            "Iteration 774, loss = 0.00283202\n",
            "Iteration 775, loss = 0.00283216\n",
            "Iteration 776, loss = 0.00282515\n",
            "Iteration 777, loss = 0.00282664\n",
            "Iteration 778, loss = 0.00278954\n",
            "Iteration 779, loss = 0.00282538\n",
            "Iteration 780, loss = 0.00281244\n",
            "Iteration 781, loss = 0.00276526\n",
            "Iteration 782, loss = 0.00278315\n",
            "Iteration 783, loss = 0.00276892\n",
            "Iteration 784, loss = 0.00292755\n",
            "Iteration 785, loss = 0.00273641\n",
            "Iteration 786, loss = 0.00288625\n",
            "Iteration 787, loss = 0.00285502\n",
            "Iteration 788, loss = 0.00283520\n",
            "Iteration 789, loss = 0.00271522\n",
            "Iteration 790, loss = 0.00274182\n",
            "Iteration 791, loss = 0.00277181\n",
            "Iteration 792, loss = 0.00272592\n",
            "Iteration 793, loss = 0.00265976\n",
            "Iteration 794, loss = 0.00267401\n",
            "Iteration 795, loss = 0.00276654\n",
            "Iteration 796, loss = 0.00264640\n",
            "Iteration 797, loss = 0.00268448\n",
            "Iteration 798, loss = 0.00268308\n",
            "Iteration 799, loss = 0.00267415\n",
            "Iteration 800, loss = 0.00267006\n",
            "Iteration 801, loss = 0.00264149\n",
            "Iteration 802, loss = 0.00263431\n",
            "Iteration 803, loss = 0.00262895\n",
            "Iteration 804, loss = 0.00265055\n",
            "Iteration 805, loss = 0.00265113\n",
            "Iteration 806, loss = 0.00263912\n",
            "Iteration 807, loss = 0.00259637\n",
            "Iteration 808, loss = 0.00259992\n",
            "Iteration 809, loss = 0.00264442\n",
            "Iteration 810, loss = 0.00258128\n",
            "Iteration 811, loss = 0.00266177\n",
            "Iteration 812, loss = 0.00274918\n",
            "Iteration 813, loss = 0.00259010\n",
            "Iteration 814, loss = 0.00258581\n",
            "Iteration 815, loss = 0.00257341\n",
            "Iteration 816, loss = 0.00253283\n",
            "Iteration 817, loss = 0.00251548\n",
            "Iteration 818, loss = 0.00254610\n",
            "Iteration 819, loss = 0.00253507\n",
            "Iteration 820, loss = 0.00255402\n",
            "Iteration 821, loss = 0.00247472\n",
            "Iteration 822, loss = 0.00256486\n",
            "Iteration 823, loss = 0.00260037\n",
            "Iteration 824, loss = 0.00244811\n",
            "Iteration 825, loss = 0.00253673\n",
            "Iteration 826, loss = 0.00246998\n",
            "Iteration 827, loss = 0.00250990\n",
            "Iteration 828, loss = 0.00247864\n",
            "Iteration 829, loss = 0.00249620\n",
            "Iteration 830, loss = 0.00249239\n",
            "Iteration 831, loss = 0.00245854\n",
            "Iteration 832, loss = 0.00247294\n",
            "Iteration 833, loss = 0.00242100\n",
            "Iteration 834, loss = 0.00243827\n",
            "Iteration 835, loss = 0.00242005\n",
            "Iteration 836, loss = 0.00241664\n",
            "Iteration 837, loss = 0.00241757\n",
            "Iteration 838, loss = 0.00239117\n",
            "Iteration 839, loss = 0.00238054\n",
            "Iteration 840, loss = 0.00250786\n",
            "Iteration 841, loss = 0.00239841\n",
            "Iteration 842, loss = 0.00243183\n",
            "Iteration 843, loss = 0.00237350\n",
            "Iteration 844, loss = 0.00240035\n",
            "Iteration 845, loss = 0.00237642\n",
            "Iteration 846, loss = 0.00236976\n",
            "Iteration 847, loss = 0.00235054\n",
            "Iteration 848, loss = 0.00235863\n",
            "Iteration 849, loss = 0.00242669\n",
            "Iteration 850, loss = 0.00233965\n",
            "Iteration 851, loss = 0.00237156\n",
            "Iteration 852, loss = 0.00229452\n",
            "Iteration 853, loss = 0.00236266\n",
            "Iteration 854, loss = 0.00238759\n",
            "Iteration 855, loss = 0.00234319\n",
            "Iteration 856, loss = 0.00228467\n",
            "Iteration 857, loss = 0.00229034\n",
            "Iteration 858, loss = 0.00230252\n",
            "Iteration 859, loss = 0.00226481\n",
            "Iteration 860, loss = 0.00232949\n",
            "Iteration 861, loss = 0.00229624\n",
            "Iteration 862, loss = 0.00229674\n",
            "Iteration 863, loss = 0.00227372\n",
            "Iteration 864, loss = 0.00226289\n",
            "Iteration 865, loss = 0.00225463\n",
            "Iteration 866, loss = 0.00225236\n",
            "Iteration 867, loss = 0.00225455\n",
            "Iteration 868, loss = 0.00232785\n",
            "Iteration 869, loss = 0.00222980\n",
            "Iteration 870, loss = 0.00227396\n",
            "Iteration 871, loss = 0.00222280\n",
            "Iteration 872, loss = 0.00222064\n",
            "Iteration 873, loss = 0.00221373\n",
            "Iteration 874, loss = 0.00224172\n",
            "Iteration 875, loss = 0.00219715\n",
            "Iteration 876, loss = 0.00219469\n",
            "Iteration 877, loss = 0.00222484\n",
            "Iteration 878, loss = 0.00220384\n",
            "Iteration 879, loss = 0.00221557\n",
            "Iteration 880, loss = 0.00218906\n",
            "Iteration 881, loss = 0.00215598\n",
            "Iteration 882, loss = 0.00216100\n",
            "Iteration 883, loss = 0.00215526\n",
            "Iteration 884, loss = 0.00217628\n",
            "Iteration 885, loss = 0.00216578\n",
            "Iteration 886, loss = 0.00226591\n",
            "Iteration 887, loss = 0.00218996\n",
            "Iteration 888, loss = 0.00215723\n",
            "Iteration 889, loss = 0.00212713\n",
            "Iteration 890, loss = 0.00212489\n",
            "Iteration 891, loss = 0.00215441\n",
            "Iteration 892, loss = 0.00215045\n",
            "Iteration 893, loss = 0.00214224\n",
            "Iteration 894, loss = 0.00220428\n",
            "Iteration 895, loss = 0.00208654\n",
            "Iteration 896, loss = 0.00214711\n",
            "Iteration 897, loss = 0.00214180\n",
            "Iteration 898, loss = 0.00210090\n",
            "Iteration 899, loss = 0.00206978\n",
            "Iteration 900, loss = 0.00208605\n",
            "Iteration 901, loss = 0.00211427\n",
            "Iteration 902, loss = 0.00206195\n",
            "Iteration 903, loss = 0.00211313\n",
            "Iteration 904, loss = 0.00210626\n",
            "Iteration 905, loss = 0.00209518\n",
            "Iteration 906, loss = 0.00203694\n",
            "Iteration 907, loss = 0.00210362\n",
            "Iteration 908, loss = 0.00207858\n",
            "Iteration 909, loss = 0.00208768\n",
            "Iteration 910, loss = 0.00205940\n",
            "Iteration 911, loss = 0.00203457\n",
            "Iteration 912, loss = 0.00203795\n",
            "Iteration 913, loss = 0.00207071\n",
            "Iteration 914, loss = 0.00199814\n",
            "Iteration 915, loss = 0.00202747\n",
            "Iteration 916, loss = 0.00201506\n",
            "Iteration 917, loss = 0.00201514\n",
            "Iteration 918, loss = 0.00199982\n",
            "Iteration 919, loss = 0.00198501\n",
            "Iteration 920, loss = 0.00198465\n",
            "Iteration 921, loss = 0.00199313\n",
            "Iteration 922, loss = 0.00204836\n",
            "Iteration 923, loss = 0.00201049\n",
            "Iteration 924, loss = 0.00200926\n",
            "Iteration 925, loss = 0.00204762\n",
            "Iteration 926, loss = 0.00195986\n",
            "Iteration 927, loss = 0.00197824\n",
            "Iteration 928, loss = 0.00195382\n",
            "Iteration 929, loss = 0.00200003\n",
            "Iteration 930, loss = 0.00209088\n",
            "Iteration 931, loss = 0.00194146\n",
            "Iteration 932, loss = 0.00196256\n",
            "Iteration 933, loss = 0.00194385\n",
            "Iteration 934, loss = 0.00198331\n",
            "Iteration 935, loss = 0.00189917\n",
            "Iteration 936, loss = 0.00191128\n",
            "Iteration 937, loss = 0.00190189\n",
            "Iteration 938, loss = 0.00193666\n",
            "Iteration 939, loss = 0.00191316\n",
            "Iteration 940, loss = 0.00192886\n",
            "Iteration 941, loss = 0.00191507\n",
            "Iteration 942, loss = 0.00192534\n",
            "Iteration 943, loss = 0.00185043\n",
            "Iteration 944, loss = 0.00191442\n",
            "Iteration 945, loss = 0.00190900\n",
            "Iteration 946, loss = 0.00187244\n",
            "Iteration 947, loss = 0.00185972\n",
            "Iteration 948, loss = 0.00187199\n",
            "Iteration 949, loss = 0.00185655\n",
            "Iteration 950, loss = 0.00185330\n",
            "Iteration 951, loss = 0.00185940\n",
            "Iteration 952, loss = 0.00188664\n",
            "Iteration 953, loss = 0.00184247\n",
            "Iteration 954, loss = 0.00183356\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-6 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-6 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-6 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-6 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-6 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-6 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-6 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-6 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-6 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-6 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-6 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-6 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-6 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-6 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-6 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsao_credit = rede_neural_credit.predict(X_credit_teste)\n",
        "previsao_credit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVWUQMGchDmT",
        "outputId": "2930e5db-f8ff-4be1-9b8f-b63adeec4b89"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_credit_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySaOX7FhhRpK",
        "outputId": "fa9ba489-5b1d-4c5c-b311-326767134e85"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_credit_teste, previsao_credit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KLAKksPhNAT",
        "outputId": "0c02a2c5-b43f-4881-ce7e-dca77df910de"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.996"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConfusionMatrix(rede_neural_credit)\n",
        "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
        "cm.score(X_credit_teste, y_credit_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "fUxvS8JChPN6",
        "outputId": "5abd51d7-ab18-4575-fe00-0139e8b12654"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.996"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_credit_teste, previsao_credit))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5itruUuPhahF",
        "outputId": "a256e201-3716-423a-d3d0-ca889196949e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       436\n",
            "           1       0.98      0.98      0.98        64\n",
            "\n",
            "    accuracy                           1.00       500\n",
            "   macro avg       0.99      0.99      0.99       500\n",
            "weighted avg       1.00      1.00      1.00       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base - Census"
      ],
      "metadata": {
        "id": "4LSBlI8-iZph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(caminho + 'census.pkl', 'rb') as f:\n",
        "  X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste = pickle.load(f)"
      ],
      "metadata": {
        "id": "89DZBHM9iV1B"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_census_treinamento.shape, y_census_treinamento.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffs6hI_vibpp",
        "outputId": "2323f035-e1cc-43df-ff57-c1c3cecaffc4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((27676, 108), (27676,))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_census_teste.shape, y_census_teste.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qV33hn1id5S",
        "outputId": "110ec270-26ae-4dc8-86a0-238be0767edb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4885, 108), (4885,))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rede_neural_census = MLPClassifier(verbose=True,max_iter=1000, tol=0.0000100,hidden_layer_sizes=(60,100,60))\n",
        "rede_neural_census.fit(X_census_treinamento, y_census_treinamento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JZHMzlQTivDy",
        "outputId": "b3a37161-b2ac-40c2-9ed0-651873cc6d0b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.38205637\n",
            "Iteration 2, loss = 0.32447014\n",
            "Iteration 3, loss = 0.31240842\n",
            "Iteration 4, loss = 0.30410477\n",
            "Iteration 5, loss = 0.29981820\n",
            "Iteration 6, loss = 0.29608485\n",
            "Iteration 7, loss = 0.29005978\n",
            "Iteration 8, loss = 0.28716116\n",
            "Iteration 9, loss = 0.28435387\n",
            "Iteration 10, loss = 0.28069681\n",
            "Iteration 11, loss = 0.27687544\n",
            "Iteration 12, loss = 0.27503011\n",
            "Iteration 13, loss = 0.27098719\n",
            "Iteration 14, loss = 0.26847509\n",
            "Iteration 15, loss = 0.26578991\n",
            "Iteration 16, loss = 0.26245639\n",
            "Iteration 17, loss = 0.25919316\n",
            "Iteration 18, loss = 0.25410171\n",
            "Iteration 19, loss = 0.25454860\n",
            "Iteration 20, loss = 0.25030922\n",
            "Iteration 21, loss = 0.24900744\n",
            "Iteration 22, loss = 0.24467234\n",
            "Iteration 23, loss = 0.24194542\n",
            "Iteration 24, loss = 0.23842242\n",
            "Iteration 25, loss = 0.23785475\n",
            "Iteration 26, loss = 0.23202716\n",
            "Iteration 27, loss = 0.23141337\n",
            "Iteration 28, loss = 0.22904900\n",
            "Iteration 29, loss = 0.22621440\n",
            "Iteration 30, loss = 0.22288814\n",
            "Iteration 31, loss = 0.22214738\n",
            "Iteration 32, loss = 0.21740155\n",
            "Iteration 33, loss = 0.21513874\n",
            "Iteration 34, loss = 0.21291771\n",
            "Iteration 35, loss = 0.21236765\n",
            "Iteration 36, loss = 0.21164789\n",
            "Iteration 37, loss = 0.20753260\n",
            "Iteration 38, loss = 0.20461817\n",
            "Iteration 39, loss = 0.20186863\n",
            "Iteration 40, loss = 0.20006746\n",
            "Iteration 41, loss = 0.20142775\n",
            "Iteration 42, loss = 0.20126134\n",
            "Iteration 43, loss = 0.19534986\n",
            "Iteration 44, loss = 0.19645327\n",
            "Iteration 45, loss = 0.19013300\n",
            "Iteration 46, loss = 0.18931602\n",
            "Iteration 47, loss = 0.18636109\n",
            "Iteration 48, loss = 0.18640091\n",
            "Iteration 49, loss = 0.18444540\n",
            "Iteration 50, loss = 0.18447399\n",
            "Iteration 51, loss = 0.18009809\n",
            "Iteration 52, loss = 0.17987646\n",
            "Iteration 53, loss = 0.18004934\n",
            "Iteration 54, loss = 0.17660098\n",
            "Iteration 55, loss = 0.17485489\n",
            "Iteration 56, loss = 0.17300627\n",
            "Iteration 57, loss = 0.17271994\n",
            "Iteration 58, loss = 0.17275409\n",
            "Iteration 59, loss = 0.17263314\n",
            "Iteration 60, loss = 0.17146835\n",
            "Iteration 61, loss = 0.16780394\n",
            "Iteration 62, loss = 0.16602517\n",
            "Iteration 63, loss = 0.16589806\n",
            "Iteration 64, loss = 0.16482633\n",
            "Iteration 65, loss = 0.16314009\n",
            "Iteration 66, loss = 0.16408245\n",
            "Iteration 67, loss = 0.16110867\n",
            "Iteration 68, loss = 0.15952480\n",
            "Iteration 69, loss = 0.15813492\n",
            "Iteration 70, loss = 0.15966231\n",
            "Iteration 71, loss = 0.15841537\n",
            "Iteration 72, loss = 0.15852829\n",
            "Iteration 73, loss = 0.15592392\n",
            "Iteration 74, loss = 0.15719138\n",
            "Iteration 75, loss = 0.15505806\n",
            "Iteration 76, loss = 0.15056305\n",
            "Iteration 77, loss = 0.15388156\n",
            "Iteration 78, loss = 0.15003992\n",
            "Iteration 79, loss = 0.14933514\n",
            "Iteration 80, loss = 0.14852091\n",
            "Iteration 81, loss = 0.14770841\n",
            "Iteration 82, loss = 0.14987124\n",
            "Iteration 83, loss = 0.14656782\n",
            "Iteration 84, loss = 0.14913717\n",
            "Iteration 85, loss = 0.14558425\n",
            "Iteration 86, loss = 0.14424879\n",
            "Iteration 87, loss = 0.14287546\n",
            "Iteration 88, loss = 0.14228170\n",
            "Iteration 89, loss = 0.14378134\n",
            "Iteration 90, loss = 0.14265239\n",
            "Iteration 91, loss = 0.13958348\n",
            "Iteration 92, loss = 0.13757081\n",
            "Iteration 93, loss = 0.14042475\n",
            "Iteration 94, loss = 0.13961695\n",
            "Iteration 95, loss = 0.13940270\n",
            "Iteration 96, loss = 0.13557377\n",
            "Iteration 97, loss = 0.13506766\n",
            "Iteration 98, loss = 0.13438733\n",
            "Iteration 99, loss = 0.13965612\n",
            "Iteration 100, loss = 0.13790247\n",
            "Iteration 101, loss = 0.13372325\n",
            "Iteration 102, loss = 0.13647117\n",
            "Iteration 103, loss = 0.13227824\n",
            "Iteration 104, loss = 0.13128014\n",
            "Iteration 105, loss = 0.13083007\n",
            "Iteration 106, loss = 0.12659234\n",
            "Iteration 107, loss = 0.12708625\n",
            "Iteration 108, loss = 0.13232635\n",
            "Iteration 109, loss = 0.12831096\n",
            "Iteration 110, loss = 0.12692539\n",
            "Iteration 111, loss = 0.12831792\n",
            "Iteration 112, loss = 0.13029421\n",
            "Iteration 113, loss = 0.12870733\n",
            "Iteration 114, loss = 0.12767007\n",
            "Iteration 115, loss = 0.12739074\n",
            "Iteration 116, loss = 0.12618269\n",
            "Iteration 117, loss = 0.12432528\n",
            "Iteration 118, loss = 0.12302794\n",
            "Iteration 119, loss = 0.12707918\n",
            "Iteration 120, loss = 0.12349450\n",
            "Iteration 121, loss = 0.12062314\n",
            "Iteration 122, loss = 0.12396149\n",
            "Iteration 123, loss = 0.12484413\n",
            "Iteration 124, loss = 0.12205551\n",
            "Iteration 125, loss = 0.12703966\n",
            "Iteration 126, loss = 0.12233668\n",
            "Iteration 127, loss = 0.11895773\n",
            "Iteration 128, loss = 0.12273684\n",
            "Iteration 129, loss = 0.11840684\n",
            "Iteration 130, loss = 0.12083415\n",
            "Iteration 131, loss = 0.12131592\n",
            "Iteration 132, loss = 0.12446275\n",
            "Iteration 133, loss = 0.12088877\n",
            "Iteration 134, loss = 0.11648928\n",
            "Iteration 135, loss = 0.11778265\n",
            "Iteration 136, loss = 0.11595277\n",
            "Iteration 137, loss = 0.11754532\n",
            "Iteration 138, loss = 0.11705699\n",
            "Iteration 139, loss = 0.11502331\n",
            "Iteration 140, loss = 0.11316377\n",
            "Iteration 141, loss = 0.11178670\n",
            "Iteration 142, loss = 0.10981585\n",
            "Iteration 143, loss = 0.11491710\n",
            "Iteration 144, loss = 0.11397012\n",
            "Iteration 145, loss = 0.11474906\n",
            "Iteration 146, loss = 0.11435802\n",
            "Iteration 147, loss = 0.11296087\n",
            "Iteration 148, loss = 0.11819952\n",
            "Iteration 149, loss = 0.11058598\n",
            "Iteration 150, loss = 0.11557109\n",
            "Iteration 151, loss = 0.11311698\n",
            "Iteration 152, loss = 0.11322181\n",
            "Iteration 153, loss = 0.11364054\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(60, 100, 60), max_iter=1000, tol=1e-05,\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-9 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-9 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-9 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-9 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-9 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-9 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-9 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-9 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-9 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-9 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-9 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-9 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-9 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-9 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-9 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-9 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(60, 100, 60), max_iter=1000, tol=1e-05,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(60, 100, 60), max_iter=1000, tol=1e-05,\n",
              "              verbose=True)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsao_census = rede_neural_census.predict(X_census_teste)"
      ],
      "metadata": {
        "id": "QWwE2rYojwby"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_census_teste, previsao_census)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JnsfT1Uj07p",
        "outputId": "ab811a00-6c1a-452d-d94d-575618bbe362"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8182190378710338"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConfusionMatrix(rede_neural_census)\n",
        "cm.fit(X_census_treinamento, y_census_treinamento)\n",
        "cm.score(X_census_teste, y_census_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "q5B90Kehj5Fq",
        "outputId": "33d70456-bb94-4403-fb1b-d78595f4563f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8182190378710338"
            ]
          },
          "metadata": {},
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_census_teste, previsao_census))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e46U5WLumiLN",
        "outputId": "6f6f03ad-e779-48e4-e57e-2d4f6b03a0fc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.88      0.88      0.88      3693\n",
            "        >50K       0.63      0.63      0.63      1192\n",
            "\n",
            "    accuracy                           0.82      4885\n",
            "   macro avg       0.75      0.75      0.75      4885\n",
            "weighted avg       0.82      0.82      0.82      4885\n",
            "\n"
          ]
        }
      ]
    }
  ]
}